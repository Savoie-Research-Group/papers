
# Learning Stability Scores from Computationally Generated Kinetic Data

This directory contained all the data, python scripts and pretrained models to train, test, predict, and reproduce the results from the machine learning and data analysis sections 2.2, 2.3, SI 1.4, and SI 4 of the chemrxiv preprint of the paper: https://doi.org/10.26434/chemrxiv-2023-0pjxv


## Author(s)

- Veerupaksh Singla | [singla2@purdue.edu](mailto:singla2@purdue.edu) | [GitHub](https://github.com/veerupaksh)
- Qiyuan Zhao | [zhaoqiy@med.umich.edu](mailto:zhaoqiy@med.umich.edu) | [GitHub](https://github.com/zhaoqy1996)
- Brett M. Savoie (Corresponding Author) | [bsavoie@purdue.edu](mailto:bsavoie@purdue.edu) | [GitHub](https://github.com/Savoie-Research-Group)

## Installation
The code is tested on the versions of software and libraries listed here. May or may not work with other versions.

#### Nvidia GPU support
| Model framework       | CUDA Version | cuDNN Version |
| :-------------------- | :----------- | :------------ |
| TensorFlow (Our MLP)  | 11.2.0       | 8.1           |
| PyTorch (Chemprop)    | 11.7.0       | 8.6           |

#### Setting up Python environment
| Conda version | Python version |
| :------------ | :------------- |
| 4.9.2         | 3.10.6         |

To install conda environment, run

```bash
  conda env create --file pythonML_environment.yml
```

To activate conda environment, run

```bash
  conda activate pythonML
```

#### Cloning Chemprop ([link](https://github.com/chemprop/chemprop))

| Commit version | 442a1602b670f173de166f987eab64396571ee98 ([link](https://github.com/chemprop/chemprop/commit/442a1602b670f173de166f987eab64396571ee98)) |
| :-------------------- | :----------- |

To clone the Chemprop version used, run

```bash
  git clone https://github.com/chemprop/chemprop.git
  cd chemprop
  git checkout 442a1602b670f173de166f987eab64396571ee98
```

## Directories' description

#### ./data
All the alkane half life data needed to train and test all of the 5 splits on both our MLP and Chemprop models. Also contains the data generated by the material design genetic algorithm and needed to make Figure 5 (b).

| Name | Description |
| :--- | :---------- |
| splits | Contains train:test splits for all split types. The \*.npy files are for the MLP models and the \*.csv files are for the Chemprop models. |
| alk_smi_hl_dict_secs_hl_prune_till_c17_32421_vals.json | Dictionary containing all the SMILES:half_lives data generated and used for model(s) training and testing |
| alkanes_till_c17_canon.txt | JSON dictionary contaning Rdkit canocalized achiral SMILES of all structural isomers of all acyclic alkanes from $C_2H_6$ through $C_{17}H_{36}$ |
| alkanes_till_c17_canon_fp_2048_int8.p | pickled dictionary containing all the SMILES:2048_int8_fingerprint data for all SMILES in alkanes_till_c17_canon.txt |
| mlp_accuracy.json, chemprop_accuracy.json | Pairwise [training, testing] accuracies of stability score prediction across all the 5 splits for the MLP and Chemprop models |
| genetic_mlp_run_all.p, genetic_mlp_run_best.p, genetic_chemprop_run_all.p, genetic_chemprop_run_best.p | All and best genetic algorithm runs for MLP and Chemprop models needed to plot genetic_c17_run.pdf |
| genetic_c17_run.pdf | Figure 5 (b) created using genetic_mlp_run_all.p, genetic_mlp_run_best.p, genetic_chemprop_run_all.p, genetic_chemprop_run_best.p and the code in ./genetic_inspired_algo_scripts |

#### ./pretrained_models

All the MLP and Chemprop models final weights across all the splits; may be directly used to predict on alkane data.

In ./pretrained_models/mlp_models, all data directories are structures as:

| Name | Description |
| :--- | :---------- |
| log.txt | Training log. Used to debug and also find the best model with the lowest validation error across all epochs. |
| train_val_loss.png | Learning curve |
| best_model_epoch_val_loss.json | Best validation epoch and the corresponding validation loss |
| model_\*.h5 | Pickle file for the best model with lowest validation loss (from best_model_epoch_val_loss.json) which is used for prediction on testing data or new data |
| y_pred_test.npy, y_pred_train.npy | Predictions on the testing and training data made using model_\*.h5 |

In ./pretrained_models/chemprop_models, all data directories are structures as:

| Name | Description |
| :--- | :---------- |
| args.json | Input training arguments parsed by Chemprop (defaults + custom) |
| quiet.log, verbose.log | Training logs. Used to debug and to find the epoch with the lowest validation error. |
| fold_0/model_0/model.pt | Checkpoint file for the best model with lowest validation loss automatically generated by Chemprop which is used for prediction on testing data or new data |
| fold_0/test_scores.json | Testing loss auto generated by Chemprop |
| test_scores.csv | More testing metrics auto generated by Chemprop |
| test_smi_pred.csv, train_val_smi_pred.csv | Predictions on the testing and training data made using fold_0/model_0/model.pt |

#### ./ml_training_scripts

All python scripts necessary to preprocess the data, train and test the models, and the necessary modifications made to train Chemprop using the custom hinge_loss function. The following table is in the order in which it is suggested to run the scripts.

| Name | Description |
| :--- | :---------- |
| 0_0_custom_hinge_loss_tf_pytorch.py | Python codes for the custom hinge loss functions for both our MLP (TensorFlow + Keras) and Chemprop (PyTorch) |
| 0_1_additions_to_chemprop.py | Descriptive file with code + comments on how to modify Chemprop |
| 1_generate_fingerprints_pickle.py | Generate the 2048_int8_fingerprint data for all SMILES in ./data/alkanes_till_c17_canon.txt and store as the pickle file ./data/alkanes_till_c17_canon_fp_2048_int8.p |
| 2_create_splits.py | Create train:test splits for all split types and store in ./data/splits |
| 3_0_train_mlp.py | Train our MLP model for a given split type. e.g. for the random split, run `python train_mlp.py --split-dir '../data/splits/random_90-10_train-test_split' --model-dir '../pretrained_models/mlp_models/random_90-10_train-test_split' > '../pretrained_models/mlp_models/random_90-10_train-test_split/log.txt`. Creates the log.txt and train_val_loss.png files inside each split directory in ./pretrained_models/mlp_models. |
| 3_1_train_hyperopt_chemprop_commands.txt | Descriptive text file containing commands for hyperparameter optimization and training Chemprop on a particular split. Creates fold_0, args.json, quiet.log, verbose.log, and test_scores.csv files inside each split directory in ./pretrained_models/chemprop_models  |
| 4_0_mlp_find_best_models_and_pred_all_splits.py | Analyze the MLP training log.txt file, find and save the best model with lowest validation error, and use this best model to predict on the training ans testing data for all splits. Creates the best_model_epoch_val_loss.json, y_pred_test.npy, and y_pred_train.npy files inside each split directory in ./pretrained_models/mlp_models. |
| 4_1_chemprop_pred_all_splits_commands.txt | Descriptive text file containing commands to predict the trained Chemprop models on training and testing data across all splits. Creates the test_smi_pred.csv, and train_val_smi_pred.csv files inside each split directory in ./pretrained_models/chemprop_models |
| 5_calc_accuracy_all_splits.py | Calculate train and test accuracy for both MLP and Chemprop models across all splits and store in ./data as mlp_accuracy.json, and chemprop_accuracy.json |

#### ./genetic_inspired_algo_scripts

Run the genetic algorith to find stable alkanes using the models trained on split (v) - alkanes with length â‰¤ 16 in training set and alkanes with length == 17 in testing set, as descibed in the manuscript.

| Name | Description |
| :--- | :---------- |
| 0_run_genetic_constant_length_graph_based.py | Run the genetic algorithm using both the MLP and Chemprop pretrained models and save the data in the genetic_mlp_run_all.p, genetic_mlp_run_best.p, genetic_chemprop_run_all.p, genetic_chemprop_run_best.p file in ./data directory. |
| 1_plot_genetic.py | Reproduce the Figure 5 (b) plot from the manuscript using the pickle files generated by 0_run_genetic_constant_length_graph_based.py and save it as genetic_c17_run.pdf |
